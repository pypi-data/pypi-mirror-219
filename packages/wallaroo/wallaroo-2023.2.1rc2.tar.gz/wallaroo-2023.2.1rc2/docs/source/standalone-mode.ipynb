{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standalone Mode\n",
    "\n",
    "The Wallaroo Engine can be used in a single-container standalone mode, without the full Wallaroo Kubernetes stack. This page will cover the intended workflow: \n",
    "\n",
    " * [Install prerequisites](#Install-prerequisites) for local use\n",
    " * [Install the engine image](#Install-the-engine-image) for local use\n",
    " * [Use the SDK to generate models and configuration files](#Use-the-SDK-to-generate-models-and-configuration-files) for models and model configuration to the engine\n",
    " * [Launch standalone engine](#Launch-standalone-engine)\n",
    " * [Perform inference](#Perform-inference)\n",
    " * Call the [HTTP inference API](HTTP-Inference-API) directly \n",
    " * [Integration tests](#Integration-tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Prerequisites\n",
    "\n",
    "* Docker CLI\n",
    "* Python3, or Jupyter. This notebook can also be used if Jupyter is preferred.  Python 3.8.6 has been tested but other versions may work. Installing the `.whl` file will bring in transitive dependencies.\n",
    "\n",
    "```shell\n",
    "pip install standalone-engine/wallaroo-0.0.24-py3-none-any.whl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the Engine image\n",
    "\n",
    "The installation directory contains:\n",
    "* `engine-image.tgz` - A compressed TAR file of the `x86-64` Wallaroo standalone engine Docker image. The image is based off the [busybox:glibc](https://hub.docker.com/_/busybox/) image, which is derived from [Debian GLIBC](https://packages.debian.org/search?searchon=names&exact=1&suite=all&section=all&keywords=libc6).\n",
    "* `wallaroo-*.whl` - The Wallaroo Python SDK\n",
    "\n",
    "A desktop with working Docker CLI is required. First load the image into the local system and then list images to observe it was loaded:\n",
    "```shell\n",
    "$ docker load --input standalone-engine/engine-image.tgz\n",
    "\n",
    "$ docker images | head -2\n",
    "REPOSITORY                               TAG         IMAGE ID       CREATED         SIZE\n",
    "ghcr.io/wallaroolabs/standalone-mini     latest      9f393d8dd074   18 hours ago    975MB\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the SDK to generate models and configuration files\n",
    "\n",
    "The Wallaroo standalone engine requires configuration and model files placed into the appropriate directory.  The engine config is required before startup and the others can be provided any time before inference. The Wallaroo SDK enables this flow by generating the configuration files to place in this directory structure.\n",
    "\n",
    "The engine configuration file location can be set by using the `ENGINE_CONFIG_FILE` environment variable. This defaults to `/engine/config.yaml`\n",
    "\n",
    "The model and pipeline configuration directories can be set using the `EngineConfig`. They default to `/modelconfig` and `/pipelineconfig`.\n",
    "\n",
    "The directory structure to which models are expected to adhere is\n",
    "\n",
    "`/models/<model_class>/<model_name>/<file>`\n",
    "\n",
    "The top level models directory can also be configured through the `EngineConfig`\n",
    "\n",
    "Configurations in each directory _must_ be named uniquely.\n",
    "\n",
    "An example follows below, where we will write the files to an \"engine\" directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wallaroo.pipeline_config import *\n",
    "from wallaroo.model import *\n",
    "from wallaroo.model_config import *\n",
    "from wallaroo.engine_config import *\n",
    "from wallaroo.standalone_client import *\n",
    "from os import makedirs\n",
    "\n",
    "builder = PipelineConfigBuilder.as_standalone(\n",
    "    pipeline_name=\"pipeline-1\", variant_name=\"v1\"\n",
    ")\n",
    "\n",
    "model = Model.as_standalone(\"model_name\", \"model_version\", \"hello.onnx\")\n",
    "\n",
    "model_config = ModelConfig.as_standalone(model=model, runtime=\"onnx\")\n",
    "\n",
    "engine_config = EngineConfig.as_standalone(\n",
    "    cpus=1,\n",
    "    model_directory=\"/engine/model\",\n",
    "    pipeline_config_directory=\"/engine/pipelineconfig\",\n",
    "    model_config_directory=\"/engine/modelconfig\",\n",
    ")\n",
    "\n",
    "builder.add_model_step(model_config)\n",
    "\n",
    "makedirs(\"engine\", exist_ok=True)\n",
    "makedirs(\"engine/modelconfig\", exist_ok=True)\n",
    "makedirs(\"engine/pipelineconfig\", exist_ok=True)\n",
    "makedirs(\"engine/model\", exist_ok=True)\n",
    "\n",
    "with open(\"engine/modelconfig/my_model_config.yaml\", \"w\") as model_config_file:\n",
    "    model_config_file.write(model_config.to_yaml())\n",
    "\n",
    "with open(\"engine/pipelineconfig/my_pipeline_config.yaml\", \"w\") as pipeline_config_file:\n",
    "    pipeline_config_file.write(builder.config().to_yaml())\n",
    "\n",
    "with open(\"engine/engine_config.yaml\", \"w\") as engine_config_file:\n",
    "    engine_config_file.write(engine_config.to_yaml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch standalone engine\n",
    "\n",
    "As above, the container will require an environment variable pointing to the config file, a volume mount for the files, and a local port for inference. This command will run the container in the background. Docker will respond with the container ID.\n",
    "\n",
    "```shell\n",
    "$ docker run --detach --rm \\\n",
    "    --env ENGINE_CONFIG_FILE=/engine/config.yaml \\\n",
    "    --publish 29502:29502 \\\n",
    "    --volume `pwd`/engine:/engine \\\n",
    "    ghcr.io/wallaroolabs/standalone-mini:latest\n",
    "585dc0b1f8e638241886b4a9f459b8f1bfb506029bbf0eb3940ca0bf69cd61c8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the engine starts, it will continually monitor the configured directories for models and config files. In the above example, the configs were all provided but not the model.  Providing the model will cause the engine to begin listening for inference.\n",
    "\n",
    "```shell\n",
    "$ cp hello.onnx engine/model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform inference\n",
    "The SDK supports running inference by passing a Python dict as the input tensor to a standalone client's `infer` method.  The example will assume the container was launched locally as above and its port is available on `localhost:29502`.\n",
    "\n",
    "To run an inference, make a client for your model or your pipeline as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To run inference on a model\n",
    "client = StandaloneClient(\"localhost\", 29502, model=model)\n",
    "client.infer({\"tensor\": [[1,2,3,4,5]]})\n",
    "\n",
    "#To run inference on a pipeline\n",
    "client = StandaloneClient(\"localhost\", 29502, pipeline_config=builder.config())\n",
    "client.infer({\"tensor\": [[1,2,3,4,5]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standalone engine HTTP API\n",
    "\n",
    "There is a simple HTTP REST API for status and inference. The examples again assume the engine is running on `localhost:29502` as launched above. \n",
    "\n",
    "The status calls are both GET methods: anything other than a `Running` status is an error and the container logs should be examined for messages.\n",
    "\n",
    "### Model Status\n",
    "```shell\n",
    "$ curl -s localhost:29502/models | jq .\n",
    "{\n",
    "  \"models\": [\n",
    "    {\n",
    "      \"class\": \"id2\",\n",
    "      \"name\": \"ver2\",\n",
    "      \"status\": \"Running\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "### Pipeline Status\n",
    "```shell\n",
    "$ curl -s localhost:29502/pipelines | jq .\n",
    "{\n",
    "  \"pipelines\": [\n",
    "    {\n",
    "      \"id\": \"pipeline-1\",\n",
    "      \"status\": \"Running\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "###  Inference\n",
    "\n",
    "The inference call is a POST to the endpoints listed above, passing a JSON tensor as input.\n",
    "\n",
    "```shell\n",
    "$ curl -XPOST localhost:29502/pipelines/pipeline-1 \\\n",
    "    --data --data '{\"text_input\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,28,16,32,23,29,32,30,19,26,17]]}'\n",
    "    \n",
    "[{\"check_failures\":[],\"elapsed\":12453700,\"model_id\":\"id2\",\"model_version\":\"version\",\"original_data\":{\"text_input\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,28,16,32,23,29,32,30,19,26,17]]},\"outputs\":[{\"Float\":{\"data\":[0.001519620418548584],\"dim\":[1,1],\"v\":1}},{\"Float\":{\"data\":[0.9829147458076477],\"dim\":[1,1],\"v\":1}},{\"Float\":{\"data\":[0.01209956407546997],\"dim\":[1,1],\"v\":1}},{\"Float\":{\"data\":[0.000047593468480044976],\"dim\":[1,1],\"v\":1}},{\"Float\":{\"data\":[0.000020289742678869516],\"dim\":[1,1],\"v\":1}},{\"Float\":{\"data\":[0.0003197789192199707],\"dim\":[1,1],\"v\":1}},{\"Float\":{\"data\":[0.011029303073883057],\"dim\":[1,1],\"v\":1}},{\"Float\":{\"data\":[0.9975639581680298],\"dim\":[1,1],\"v\":1}},{\"Float\":{\"data\":[0.010341644287109375],\"dim\":[1,1],\"v\":1}},{\"Float\":{\"data\":[0.008038878440856934],\"dim\":[1,1],\"v\":1}},{\"Float\":{\"data\":[0.016155093908309937],\"dim\":[1,1],\"v\":1}},{\"Float\":{\"data\":[0.006236225366592407],\"dim\":[1,1],\"v\":1}},{\"Float\":{\"data\":[0.0009985864162445068],\"dim\":[1,1],\"v\":1}},{\"Float\":{\"data\":[1.7933298217905702e-26],\"dim\":[1,1],\"v\":1}},{\"Float\":{\"data\":[1.388984431455466e-27],\"dim\":[1,1],\"v\":1}}],\"pipeline_id\":\"pipeline-1\",\"time\":1639425783513}]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run integration tests\n",
    "A simple Python test fixture is provided which creates and destroys engine containers, feeds them models, and performs inference.  It also serves as a source of examples and demonstration of SDK use.\n",
    "\n",
    "1. Install the SDK and the test resources.\n",
    "\n",
    "```shell\n",
    "$ cd standalone-engine\n",
    "$ pip install wallaroo-0.0.24-py3-none-any.whl\n",
    "$ pip install -r test_requirements.txt\n",
    "```\n",
    "\n",
    "2. Run the tests\n",
    "\n",
    "```shell\n",
    "$ PERF_RESOURCES=./test_resources TEST_RESOURCES=./test_resources pytest\n",
    "================================================================================= test session starts ==================================================================================\n",
    "platform darwin -- Python 3.9.8, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
    "benchmark: 3.4.1 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\n",
    "rootdir: /Users/mnp/prj/platform/standalone_test/standalone-engine\n",
    "plugins: benchmark-3.4.1, snapshot-0.6.1\n",
    "collected 6 items\n",
    "\n",
    "tests/test_standalone_engine.py ......                                                                                                                                           [100%]\n",
    "\n",
    "================================================================================== 6 passed in 27.37s ==================================================================================\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
