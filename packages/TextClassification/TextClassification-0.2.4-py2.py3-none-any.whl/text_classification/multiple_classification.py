# -*- coding: utf-8 -*-
"""Falah-Multiple package text classification pipeline

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qhojSuDnjty1C4nLnOvEDCFZczuI6dfM
"""



#!pip install transformers==4.27.1 datasets==2.9.0

import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import torch
from torch.utils.data import DataLoader, Dataset
from sklearn.preprocessing import LabelEncoder

class CustomClassifier:
    def __init__(self, dataset_path, model_name, batch_size=16, max_length=128, num_epochs=20, learning_rate=2e-5):
        self.dataset_path = dataset_path
        self.model_name = model_name
        self.batch_size = batch_size
        self.max_length = max_length
        self.num_epochs = num_epochs
        self.learning_rate = learning_rate
        self.label_encoder = LabelEncoder()
        self.tokenizer = None
        self.model = None
        self.train_dataset = None
        self.val_dataset = None
        self.trainer = None

    def load_dataset(self):
        # Load the dataset from the CSV file
        df = pd.read_csv(self.dataset_path)
        df['encoded_label'] = self.label_encoder.fit_transform(df['label'])
        train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)
        return train_df, val_df

    def create_datasets(self, train_df, val_df):
        # Create instances of the custom dataset for training and validation
        self.train_dataset = DiseaseDataset(
            train_df['text'].tolist(), train_df['encoded_label'].tolist(), self.tokenizer, self.max_length
        )
        self.val_dataset = DiseaseDataset(
            val_df['text'].tolist(), val_df['encoded_label'].tolist(), self.tokenizer, self.max_length
        )

    def train_model(self):
        # Set up the tokenizer and model
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            self.model_name, num_labels=len(self.label_encoder.classes_)
        )

        # Create datasets
        train_df, val_df = self.load_dataset()
        self.create_datasets(train_df, val_df)

        # Set up the TrainingArguments
        training_args = TrainingArguments(
            output_dir='./results',
            num_train_epochs=self.num_epochs,
            per_device_train_batch_size=self.batch_size,
            per_device_eval_batch_size=self.batch_size,
            learning_rate=self.learning_rate,
            logging_dir='./logs',
            logging_steps=100,
            evaluation_strategy='epoch',
            save_strategy='epoch',
            disable_tqdm=False,
        )

        # Create the Trainer
        self.trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=self.train_dataset,
            eval_dataset=self.val_dataset,
            tokenizer=self.tokenizer,
            compute_metrics=self.compute_metrics
        )

        # Train the model
        self.trainer.train()

    # Rest of the code remains the same
    ...


    def compute_metrics(self, eval_pred):
        predictions, labels = eval_pred
        predictions = predictions.argmax(axis=1)
        accuracy = (predictions == labels).mean()
        return {"accuracy": accuracy}

    def predict(self, text):
        encoding = self.tokenizer.encode_plus(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        input_ids = encoding['input_ids'].flatten()
        attention_mask = encoding['attention_mask'].flatten()

        with torch.no_grad():
            inputs = {
                'input_ids': input_ids.unsqueeze(0),
                'attention_mask': attention_mask.unsqueeze(0)
            }
            outputs = self.model(**inputs)
            predictions = torch.argmax(outputs.logits, dim=1).item()
            predicted_label = self.label_encoder.inverse_transform([predictions])[0]

        return predicted_label

class DiseaseDataset(Dataset):
    def __init__(self, text, labels, tokenizer, max_length):
        self.text = text
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.text)

    def __getitem__(self, idx):
        encoding = self.tokenizer.encode_plus(
            self.text[idx],
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        input_ids = encoding['input_ids'].flatten()
        attention_mask = encoding['attention_mask'].flatten()
        label = torch.tensor(self.labels[idx])
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': label
        }

#dataset_path = "/content/diseases2.csv"
#model_name = "bert-base-uncased"

# Create an instance of the custom classifier
#classifier = CustomClassifier(dataset_path, model_name)

# Load the dataset
#train_df, val_df = classifier.load_dataset()

# Create the datasets
#classifier.create_datasets(train_df, val_df)

# Train the model
#classifier.train_model()

# Example prediction
#text = "Some example text to classify"
#predicted_label = classifier.predict(text)
#print(f"Predicted label: {predicted_label}")